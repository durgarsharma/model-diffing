#!/usr/bin/env python3
"""
Step 1.4: Verify complete setup before moving to fine-tuning
"""

import pandas as pd
import os
import json
from pathlib import Path

print("ğŸ” Verifying complete Step 1 setup...")

# Check directory structure
expected_dirs = ['data/xnli', 'data/xquad', 'models', 'results']
expected_files = [
    'data/xnli/en_train.csv',
    'data/xnli/hi_train.csv', 
    'data/xnli/en_test.csv',
    'data/xnli/hi_test.csv',
    'data/xquad/en_train.csv',
    'data/xquad/hi_train.csv',
    'data/xquad/en_test.csv', 
    'data/xquad/hi_test.csv',
    'models/model_info.json'
]

print("\nğŸ“ Checking directory structure...")
for directory in expected_dirs:
    if os.path.exists(directory):
        print(f"âœ… {directory}")
    else:
        print(f"âŒ {directory} - MISSING!")

print("\nğŸ“„ Checking required files...")
for file_path in expected_files:
    if os.path.exists(file_path):
        # Get file size
        size = os.path.getsize(file_path) / 1024  # KB
        print(f"âœ… {file_path} ({size:.1f} KB)")
    else:
        print(f"âŒ {file_path} - MISSING!")

# Load and verify datasets
print("\nğŸ“Š Dataset verification:")

datasets_info = {}

# XNLI datasets
for lang in ['en', 'hi']:
    for split in ['train', 'test']:
        file_path = f"data/xnli/{lang}_{split}.csv"
        if os.path.exists(file_path):
            df = pd.read_csv(file_path)
            datasets_info[f"xnli_{lang}_{split}"] = len(df)
            print(f"XNLI {lang.upper()} {split}: {len(df)} examples")
            
            # Check columns
            expected_cols = ['premise', 'hypothesis', 'label']
            if all(col in df.columns for col in expected_cols):
                print(f"  âœ… All required columns present")
            else:
                print(f"  âŒ Missing columns: {set(expected_cols) - set(df.columns)}")
                
            # Check for missing values
            missing = df.isnull().sum().sum()
            if missing == 0:
                print(f"  âœ… No missing values")
            else:
                print(f"  âš ï¸ {missing} missing values found")

print()

# XQuAD datasets  
for lang in ['en', 'hi']:
    for split in ['train', 'test']:
        file_path = f"data/xquad/{lang}_{split}.csv"
        if os.path.exists(file_path):
            df = pd.read_csv(file_path)
            datasets_info[f"xquad_{lang}_{split}"] = len(df)
            print(f"XQuAD {lang.upper()} {split}: {len(df)} examples")
            
            # Check columns
            expected_cols = ['context', 'question', 'answer_text']
            if all(col in df.columns for col in expected_cols):
                print(f"  âœ… All required columns present")
            else:
                print(f"  âŒ Missing columns: {set(expected_cols) - set(df.columns)}")
                
            # Check for missing values (answer_text can be empty for some QA)
            missing = df[['context', 'question']].isnull().sum().sum()
            if missing == 0:
                print(f"  âœ… No missing values in context/question")
            else:
                print(f"  âš ï¸ {missing} missing values in context/question")

# Load model info
print("\nğŸ¤– Model configuration:")
if os.path.exists('models/model_info.json'):
    with open('models/model_info.json', 'r') as f:
        model_info = json.load(f)
    
    for key, value in model_info.items():
        print(f"  {key}: {value}")
else:
    print("  âŒ Model info file missing!")

# Summary
print(f"\nğŸ“ˆ Setup Summary:")
print(f"âœ… Total XNLI training examples: EN={datasets_info.get('xnli_en_train', 0)}, HI={datasets_info.get('xnli_hi_train', 0)}")
print(f"âœ… Total XNLI test examples: EN={datasets_info.get('xnli_en_test', 0)}, HI={datasets_info.get('xnli_hi_test', 0)}")
print(f"âœ… Total XQuAD training examples: EN={datasets_info.get('xquad_en_train', 0)}, HI={datasets_info.get('xquad_hi_train', 0)}")
print(f"âœ… Total XQuAD test examples: EN={datasets_info.get('xquad_en_test', 0)}, HI={datasets_info.get('xquad_hi_test', 0)}")

# Check if we're ready for Step 2
all_files_exist = all(os.path.exists(f) for f in expected_files)

if all_files_exist:
    print(f"\nğŸ‰ STEP 1 COMPLETE!")
    print(f"âœ… All datasets downloaded and processed")
    print(f"âœ… Base models loaded and tested") 
    print(f"âœ… Directory structure created")
    print(f"\nğŸš€ Ready to proceed to STEP 2: Fine-tuning!")
    
    # Save setup verification
    verification_info = {
        'step_1_complete': True,
        'datasets': datasets_info,
        'all_files_present': all_files_exist,
        'verification_timestamp': pd.Timestamp.now().isoformat()
    }
    
    with open('results/step1_verification.json', 'w') as f:
        json.dump(verification_info, f, indent=2)
    
    print(f"ğŸ’¾ Verification saved to results/step1_verification.json")
    
else:
    print(f"\nâŒ SETUP INCOMPLETE!")
    print(f"Please check the missing files above and re-run the previous steps.")
